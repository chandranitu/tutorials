Entire code is in DS code (ML,DL, python)
sudo apt-get install python3-tk  for gui in pycharm
------------
seed
hypothesis testing-
Error types
RoC curve
confusion matrix
trap
overfitting/underfitting
F1 score
L1 &L2 Regularization
PCA
confounding
LogLoss evaluation
Sigmoid Fn
Entropy
Elbow method
FF transform
Distribution functions
Hidden markov


# Data Preprocessing
	-iloc
	-data wrangling (data munging)
	-missing value filling
	-imputer (sklearn.preprocessing import Imputer)
	-LabelEncoder


#regression
	-Regression
	-SimpleLinearRegression
	-Multiple Linear Regression
	-PolynomialRegression
	-Support Vector Regression (SVR)
	-DecisionTreeRegression
	-RandomForestRegression

#Classification
	-Classification
	-LogisticRegression
	-K-NearestNeighbors(K-NN)
	-SupportVectorMachine(SVM)
	-KernelSVM
	-NaiveBayes
	-DecisionTreeClassification
	-RandomForestClassification

#Clustering
	-K-MeansClustering
	-HierarchicalClustering

--------Deep Learning
RNN
hashing
histogram
orb
shift
pre trained model of neural n/w
autoencoder 

Experience in Machine Learning tools :  Neural Networks, CNN, LSTM, GRU, Reinforcement Learning, Ensembling, NLP, Computer Vision, Classification, Regression 

Experience in Machine Learning Libraries: Pytorch, Tensorflow, Keras, Scikit, Deeplearning4j (Basic), OpenAI 


1.DataPreprocessing
2.Regression
3.Classification
4.Clustering
5.AssociationRuleLearning
6.ReinforcementLearning
7.NLP
8.DimensionalityReduction
9.ModelSelectionBoosting
FFT



--------------------------
Two popular examples of regularization procedures for linear regression are:

    Lasso Regression: where Ordinary Least Squares is modified to also minimize the absolute sum of the coefficients (called L1 regularization).
    Ridge Regression: where Ordinary Least Squares is modified to also minimize the squared absolute sum of the coefficients (called L2 regularization).

These methods are effective to use when there is collinearity in your input values and ordinary least squares would overfit the training data.

Now that you know some techniques to learn the coefficients in a linear regression model, letâ€™s look at how we can use a model to make predictions on new data.


    Linear Assumption. Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).
    Remove Noise. Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.
    Remove Collinearity. Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.
    Gaussian Distributions. Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.
    Rescale Inputs: Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.


